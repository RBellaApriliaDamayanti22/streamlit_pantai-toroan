# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1h7Est8W6NGzAI5rmONMwxDPdbJNKB_

## Load Data
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Kuliah/E-Tourism/

!pip install sastrawi
!pip install swifter

#import library
import pandas as pd
import numpy as np
from string import punctuation
import re
import nltk
from nltk.tokenize import word_tokenize
# nltk.download('punkt')

#load data
# df = pd.read_excel("Toroan_tanpa netral.xlsx", encoding='cp1252', error_bad_lines=False)
df = pd.read_excel("Toroan_tanpa netral.xlsx")
df

df.shape

df.isnull().sum()

"""## Preprocessing

### 1. Symbol & Punctuation Removal
"""

#proses menghilangkan angka
def remove_numbers (text):
  return re.sub(r"\d+", "", text)
df['Ulasan'] = df['Ulasan'].apply(remove_numbers)
df['Ulasan']

#proses menghilangkan simbol dan emoji
def remove_text_special (text):
  text = text.replace('\\t',"").replace('\\n',"").replace('\\u',"").replace('\\',"")
  text = text.encode('ascii', 'replace').decode('ascii')
  return text.replace("http://"," ").replace("https://", " ")
df['Ulasan'] = df['Ulasan'].apply(remove_text_special)
print(df['Ulasan'])

#menghilangkan tanda baca
def remove_tanda_baca(text):
  text = ' '.join(re.sub("(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ",text).split())
  return text

df['Ulasan'] = df['Ulasan'].apply(remove_tanda_baca)
df.head(20)

"""### 2. Case Folding"""

#proses casefolding
def casefolding(Comment):
  Comment = Comment.lower()
  Comment = Comment.strip(" ")
  return Comment
df['Ulasan'] = df['Ulasan'].apply(casefolding)
df['Ulasan']

"""### 3. Tokenizing"""

#proses tokenisasi
from nltk.tokenize import TweetTokenizer
def word_tokenize(text):
  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
  return tokenizer.tokenize(text)

df['review_token'] = df['Ulasan'].apply(word_tokenize)
df['review_token']

"""### 4. Word Normalization"""

#Normalisasi kata tidak baku
normalize = pd.read_excel("Normalization Data.xlsx")

normalize_word_dict = {}

for index, row in normalize.iterrows():
  if row[0] not in normalize_word_dict:
    normalize_word_dict[row[0]] = row[1]

def normalized_term(comment):
  return [normalize_word_dict[term] if term in normalize_word_dict else term for term in comment]

df['comment_normalize'] = df['review_token'].apply(normalized_term)
df['comment_normalize'].head(20)

"""### 5. Stopwords Removal"""

#Stopword Removal
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
txt_stopwords = stopwords.words('indonesian')

def stopwords_removal(filtering) :
  filtering = [word for word in filtering if word not in txt_stopwords]
  return filtering

df['stopwords_removal'] = df['comment_normalize'].apply(stopwords_removal)
df['stopwords_removal'].head(20)

#stopword removal 2
data_stopwords = pd.read_excel("list_stopwords.xlsx")
print(data_stopwords)


def stopwords_removal2(filter) :
  filter = [word for word in filter if word not in data_stopwords]
  return filter

df['stopwords_removal_final'] = df['stopwords_removal'].apply(stopwords_removal2)
df['stopwords_removal_final'].head(20)

"""### 6. Stemming"""

#proses stem
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import string
import swifter
factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stemming (term):
  return stemmer.stem(term)

term_dict = {}

for document in df['stopwords_removal_final']:
  for term in document:
    if term not in term_dict:
      term_dict[term] = ''

for term in term_dict:
  term_dict[term] = stemming(term)

def get_stemming(document):
  return [term_dict[term] for term in document]

df['stemming'] = df['stopwords_removal_final'].swifter.apply(get_stemming)

print(df['stemming'])

df.head(20)

from google.colab import files
df.to_csv('hasil_stemming.csv')

"""### Load Data"""

# #load data
# df = pd.read_csv('/content/drive/My Drive/Kuliah/E-Tourism/hasil_stemming.csv')
# df

# df = df.drop(df.columns[[0]], axis=1)
# df

df['Label'].value_counts()

"""## Splitting Data 80% Training dan 20% Testing"""

df

from sklearn.model_selection import train_test_split
# Train Test Split Function
X = df[['stemming']]
y = df['Label']
def split_train_test(data_df, test_size=0.2, shuffle_state=True): # 80% Training & 20% Testing. Ubah nilai pada variable test_size jika ingin skala yang lain
    X_train, X_test, Y_train, Y_test = train_test_split(X, 
                                                        y,
                                                        shuffle=shuffle_state,
                                                        test_size=test_size, 
                                                        stratify=y,
                                                        random_state=42)
    print("Value counts for Train sentiments")
    print(Y_train.value_counts())
    print("Value counts for Test sentiments")
    print(Y_test.value_counts())
    print(type(X_train))
    print(type(Y_train))
    X_train = X_train.reset_index()
    X_test = X_test.reset_index()
    Y_train = Y_train.to_frame()
    Y_train = Y_train.reset_index()
    Y_test = Y_test.to_frame()
    Y_test = Y_test.reset_index()
    print(X_train.head())
    return X_train, X_test, Y_train, Y_test

# Call the train_test_split
X_train, X_test, Y_train, Y_test = split_train_test(df)

"""## Word2vec"""

from gensim.models import Word2Vec
import time

OUTPUT_FOLDER = '/content/drive/My Drive/Kuliah/E-Tourism/' #Untuk menyimpan file model, sesuaikan dengan drive sendiri. 
# Skip-gram model (sg = 1)
size = 1000
window = 3
min_count = 1
workers = 3
sg = 1

word2vec_model_file = OUTPUT_FOLDER + 'word2vec_' + str(size) + '.model'
start_time = time.time()
stemmed_tokens = pd.Series(df['stemming']).values
# Train the Word2Vec Model
w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)
print("Time taken to train word2vec model: " + str(time.time() - start_time))
w2v_model.save(word2vec_model_file)

X_train['stemming']

import numpy as np
# Load the model from the model file
sg_w2v_model = Word2Vec.load(word2vec_model_file)
# # Unique ID of the word
# print("Index of the word 'bagus':")
# # print(sg_w2v_model.wv.vocab["bagus"].index)
# # Total number of the words 
# print(len(sg_w2v_model.wv.vocab))
# # Print the size of the word2vec vector for one word
# print("Length of the vector generated for a word")
# print(len(sg_w2v_model['bagus']))
# # Get the mean for the vectors for an example review
# print("Print Vector Kata di index ke-0:")
# print(np.mean([sg_w2v_model[token] for token in df['stemming'][2]], axis=0))

'''
print("\n Print Semua Kata")
for i in range (len(df['stemming'])):
  print(np.max([sg_w2v_model[token] for token in df['stemming'][i]], axis=0))'''

#Vector Setiap Kata
stem = []
for i in range (len(X_train['stemming'])):
  for y in range(len(X_train['stemming'][i])):
    stem.append(X_train['stemming'][i][y])

#Vector Setiap Kata 
v = []
for i in stem:
  vector_w2v = np.mean(sg_w2v_model.wv.get_vector(i))
  v.append(vector_w2v)
print(v)

vector_w2v = pd.DataFrame(list(zip(stem, v)), columns =['kata', 'vector'])

vector_w2v

vector_w2v.to_csv('/content/drive/My Drive/Kuliah/E-Tourism/w2v_files.csv')

word2vec_filename = OUTPUT_FOLDER + 'vector_w2v.csv'
vector_w2v_file = vector_w2v.to_csv(word2vec_filename, index = False)

with open(word2vec_filename, 'w+') as word2vec_file:
    for index, row in X_train.iterrows():
        model_vector = (np.mean([sg_w2v_model.wv.get_vector(token) for token in row['stemming']], axis=0)).tolist()
        if index == 0:
            header = ",".join(str(ele) for ele in range(1000))
            word2vec_file.write(header)
            word2vec_file.write("\n")
        # Check if the line exists else it is vector of zeros
        if type(model_vector) is list:  
            line1 = ",".join( [str(vector_element) for vector_element in model_vector] )
        else:
            line1 = ",".join([str(0) for i in range(1000)])
        word2vec_file.write(line1)
        word2vec_file.write('\n')

"""## Modelling (SVM)"""

Y_train

Y_test

X_test

word2vec_df = pd.read_csv(word2vec_filename)
word2vec_df

word2vec_df.shape

import time
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
# Load from the filename
word2vec_df = pd.read_csv(word2vec_filename)
#Initialize the model https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5
clf_decision_word2vec = SVC()

start_time = time.time()
# Fit the model
clf_decision_word2vec.fit(word2vec_df, Y_train['Label'])
print("Time taken to fit the model with word2vec vectors: " + str(time.time() - start_time))

"""## Evaluasi Model"""

Y_test['Label']

from sklearn.metrics import classification_report
test_features_word2vec = []
for index, row in X_test.iterrows():
    model_vector = np.mean([sg_w2v_model.wv.get_vector(token) for token in row['stemming']], axis=0)
    if model_vector.dtype == 'float32':
        test_features_word2vec.append(model_vector)
    else:
        test_features_word2vec.append(np.array([0 for i in range(1000)]))
test_predictions_word2vec = clf_decision_word2vec.predict(test_features_word2vec)
print(classification_report(Y_test['Label'],test_predictions_word2vec))

test_predictions_word2vec

import matplotlib.pyplot as plt
import numpy
from sklearn import metrics


confusion_matrix = metrics.confusion_matrix(Y_test['Label'],test_predictions_word2vec)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()